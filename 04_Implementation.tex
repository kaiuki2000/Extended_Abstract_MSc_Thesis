\section{Implementation}
\label{sec:resul}

In our numerical implementation, we leveraged PennyLane \cite{Pennylane}, a Python library tailored for differentiable programming of quantum computers. Furthermore, NetworkX \cite{NetworkX} was instrumental in generating and manipulating graphs for addressing the MaxCut problem, while CVXPY \cite{cvxpy}, tailored for convex optimization, assisted in solving the semidefinite programming relaxation of MaxCut within the Goemansâ€“Williamson algorithm framework. Testing on actual quantum hardware was unavailable, limiting our evaluation to simulations on a personal computer. This constraint also impacted scalability and comprehensive testing, particularly for larger graphs. Simulations were conducted using the Adam optimizer \cite{kingma2017adam}, with learning rates fine-tuned as hyperparameters.

% Removed!
% PennyLane facilitates the execution of variational quantum circuits and their simultaneous training, akin to training a classical neural network, within the same Python environment. We utilized two PennyLane devices: \texttt{default.qubit} for smaller circuits and exact expectation values, and \texttt{lightning.qubit} for larger circuits.

% Maybe swap the order of the following two paragraphs.
Moreover, it's important to elucidate the methodologies employed for benchmarking and testing the algorithms' efficacy. Central to our evaluation are the cost and approximation ratio\footnote{The approximation ratio denotes the proportion of the achieved "cut" value to the MaxCut value.} plots, essential for comparing convergence rates and final cut values across the algorithms. Additionally, recognizing the influence of initial parameters on outcomes, we adopt statistically robust metrics such as average cut values and the best-so-far (BSF) cut value\footnote{If a dip occurs in the graph, it is adjusted to match the previous graph value, ensuring that the final result is monotonically increasing. That's what the BSF transformation does.}. Furthermore, to mitigate the impact of outliers, we incorporate the median BSF cut value in our analysis, computed from multiple training curves. Frequently, we use the average cut or approximation ratio curves generated from $10$ different random initializations. This approach enables us to evaluate the algorithm's performance on average post-training. Essentially, we assess the likelihood of obtaining favorable results following a single run.

% Maybe swap the order of this paragraph with the previous one.
In our pursuit of optimization, grid searches are conducted on hyperparameters like Adam's learning rate, the number of ansatz layers (\texttt{n\_layers}), and qubits (\texttt{n\_qubits}), albeit constrained by the unavailability of an HPC cluster. We also refine our evaluations by benchmarking against the Goemans-Williamson algorithm, offering insights into the relative performance of our algorithms compared to state-of-the-art solutions. % Additionally, we extend our comparative analysis to include commonly used quantum optimization algorithms such as QAOA and QEMC. % Through these multifaceted evaluations, we aim to provide nuanced insights into the strengths and limitations of each algorithm under consideration.